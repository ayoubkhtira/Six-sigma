from __future__ import annotations
import io
import math
import re
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, Tuple, Optional, List

import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from scipy import stats

from reportlab.lib.pagesizes import A4
from reportlab.lib.units import cm
from reportlab.lib import colors
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.platypus import (
    SimpleDocTemplate,
    Paragraph,
    Spacer,
    Table,
    TableStyle,
    Image as RLImage,
    PageBreak,
)
from reportlab.lib.utils import ImageReader


# -----------------------------
# Config Streamlit (UX)
# -----------------------------
st.set_page_config(
    page_title="Gage R&R (Cage R&R) ‚Äî Calculateur",
    page_icon="üìè",
    layout="wide",
)

CUSTOM_CSS = """
<style>
.block-container { padding-top: 1.2rem; padding-bottom: 2rem; }

.kpi-card {
  border: 1px solid rgba(49, 51, 63, 0.15);
  border-radius: 14px;
  padding: 14px 16px;
  background: rgba(255, 255, 255, 0.60);
  box-shadow: 0 6px 18px rgba(0,0,0,0.04);
}
.kpi-title { font-size: 0.85rem; opacity: 0.75; margin-bottom: 0.2rem; }
.kpi-value { font-size: 1.5rem; font-weight: 700; line-height: 1.2; }
.kpi-sub { font-size: 0.85rem; opacity: 0.75; margin-top: 0.35rem; }

.rr-card { border-radius: 16px; padding: 16px 16px; color: white; }
.rr-green { background: linear-gradient(135deg, rgba(21, 128, 61, 0.95), rgba(34, 197, 94, 0.85)); }
.rr-orange { background: linear-gradient(135deg, rgba(180, 83, 9, 0.95), rgba(245, 158, 11, 0.85)); }
.rr-red { background: linear-gradient(135deg, rgba(153, 27, 27, 0.95), rgba(239, 68, 68, 0.85)); }
.rr-card h3 { margin: 0 0 0.4rem 0; }
.rr-card p { margin: 0.2rem 0; opacity: 0.95; }

.small-note { font-size: 0.88rem; opacity: 0.75; }
hr { margin: 1.0rem 0; }
</style>
"""
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)


# -----------------------------
# Mod√®le de donn√©es / R√©sultats
# -----------------------------
@dataclass
class AnovaResult:
    p: int
    o: int
    r: int
    confidence_level: float
    anova_table: pd.DataFrame
    var_components: pd.DataFrame
    study_var: pd.DataFrame
    metrics: Dict[str, float]
    conclusion: Dict[str, str]
    f_tests: Dict[str, Tuple[float, float, float]]  # F-statistic, p-value, critical F


# -----------------------------
# Utilitaires
# -----------------------------
def _fmt(x: float, nd: int = 3) -> str:
    if x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x))):
        return "‚Äî"
    return f"{x:.{nd}f}"


def build_template(n_parts: int, n_ops: int, n_trials: int) -> pd.DataFrame:
    rows = []
    for i in range(1, n_parts + 1):
        for j in range(1, n_ops + 1):
            for t in range(1, n_trials + 1):
                rows.append({"Pi√®ce": f"P{i}", "Op√©rateur": f"O{j}", "Essai": t, "Mesure": np.nan})
    return pd.DataFrame(rows)


def normalize_imported_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    Accepte plusieurs variantes de noms de colonnes et les ram√®ne √†:
    Pi√®ce, Op√©rateur, Essai, Mesure
    """
    colmap = {}
    for c in df.columns:
        key = str(c).strip().lower()
        if key in {"pi√®ce", "piece", "part", "n¬∞ de la pi√®ce", "no de la piece", "num√©ro pi√®ce", "n¬∞ pi√®ce"}:
            colmap[c] = "Pi√®ce"
        elif key in {"op√©rateur", "operateur", "operator"}:
            colmap[c] = "Op√©rateur"
        elif key in {"essai", "trial", "rep", "r√©p√©tition", "repetition", "repeat"}:
            colmap[c] = "Essai"
        elif key in {"mesure", "measurement", "valeur", "value", "y"}:
            colmap[c] = "Mesure"
    return df.rename(columns=colmap).copy()


def convert_wide_to_long_specific(df: pd.DataFrame) -> pd.DataFrame:
    """
    Fonction sp√©cifique pour convertir le format exact du template TEMPLATE CAGE RR.xlsx
    
    Le template a cette structure:
    - Ligne 0: Noms d'op√©rateurs r√©p√©t√©s (OPERATEUR 1, OPERATEUR 1, OPERATEUR 1, OPERATEUR 2...)
    - Ligne 1: Num√©ros d'essai (ESSAI 1, ESSAI 2, ESSAI 3...)
    - Premi√®re colonne: Num√©ros de pi√®ces (1 √† 10)
    """
    # Faire une copie
    df = df.copy()
    
    # V√©rifier si le DataFrame a un MultiIndex (header=[0,1])
    if isinstance(df.columns, pd.MultiIndex):
        # Convertir le MultiIndex en colonnes normales
        # R√©cup√©rer les noms des deux premi√®res lignes
        df.columns = [f"{col[0]}_{col[1]}" if not pd.isna(col[1]) else str(col[0]) 
                     for col in df.columns]
        
        # Les donn√©es commencent √† partir de la ligne 0 car les en-t√™tes sont dans le MultiIndex
        data_start_row = 0
    else:
        # Le fichier a des en-t√™tes sur 2 lignes s√©par√©es
        # Les deux premi√®res lignes sont les en-t√™tes
        data_start_row = 2
        
        # Extraire les op√©rateurs de la premi√®re ligne (ligne 0)
        operators = []
        # La premi√®re colonne est le titre "N¬∞ de la pi√®ce"
        operators.append("Pi√®ce")
        
        # Pour chaque colonne de mesure
        for col_idx in range(1, len(df.columns)):
            if col_idx < len(df.iloc[0]):
                op_cell = str(df.iloc[0, col_idx])
            else:
                # Calculer l'op√©rateur bas√© sur la position
                op_num = ((col_idx - 1) // 3) + 1
                op_cell = f"OPERATEUR {op_num}"
            
            # Extraire le num√©ro d'op√©rateur
            op_match = re.search(r'(\d+)', op_cell)
            operators.append(f"O{op_match.group(1) if op_match else '1'}")
        
        # Extraire les essais de la deuxi√®me ligne (ligne 1)
        trials = []
        trials.append("Essai")  # Pour la premi√®re colonne
        
        for col_idx in range(1, len(df.columns)):
            if col_idx < len(df.iloc[1]):
                trial_cell = str(df.iloc[1, col_idx])
            else:
                # Calculer l'essai bas√© sur la position
                trial_num = ((col_idx - 1) % 3) + 1
                trial_cell = f"ESSAI {trial_num}"
            
            # Extraire le num√©ro d'essai
            trial_match = re.search(r'(\d+)', trial_cell)
            trials.append(trial_match.group(1) if trial_match else "1")
        
        # Cr√©er une liste de tuples (op√©rateur, essai) pour chaque colonne
        column_info = list(zip(operators, trials))
    
    # Pr√©parer les donn√©es en format long
    long_data = []
    
    if isinstance(df.columns, pd.MultiIndex):
        # Cas MultiIndex - les donn√©es commencent √† la ligne 0
        for row_idx in range(len(df)):
            # Num√©ro de pi√®ce (premi√®re colonne)
            piece_val = df.iloc[row_idx, 0]
            if pd.isna(piece_val):
                continue
            
            piece_str = str(piece_val).strip()
            
            # Pour chaque colonne de mesure (√† partir de la 2√®me colonne)
            for col_idx in range(1, len(df.columns)):
                measure_val = df.iloc[row_idx, col_idx]
                if pd.isna(measure_val):
                    continue
                
                # Extraire op√©rateur et essai du nom de colonne
                col_name = df.columns[col_idx]
                
                if '_' in col_name:
                    op_part, trial_part = col_name.split('_', 1)
                else:
                    op_part = col_name
                    trial_part = "1"
                
                # Extraire le num√©ro d'op√©rateur
                op_match = re.search(r'(\d+)', str(op_part))
                op_num = op_match.group(1) if op_match else "1"
                
                # Extraire le num√©ro d'essai
                trial_match = re.search(r'(\d+)', str(trial_part))
                trial_num = trial_match.group(1) if trial_match else "1"
                
                long_data.append({
                    "Pi√®ce": f"P{piece_str}",
                    "Op√©rateur": f"O{op_num}",
                    "Essai": int(trial_num),
                    "Mesure": float(measure_val)
                })
    else:
        # Cas avec en-t√™tes sur 2 lignes s√©par√©es
        for row_idx in range(data_start_row, len(df)):
            # Num√©ro de pi√®ce (premi√®re colonne)
            piece_val = df.iloc[row_idx, 0]
            if pd.isna(piece_val):
                continue
            
            piece_str = str(piece_val).strip()
            
            # Pour chaque colonne de mesure (√† partir de la 2√®me colonne)
            for col_idx in range(1, len(df.columns)):
                measure_val = df.iloc[row_idx, col_idx]
                if pd.isna(measure_val):
                    continue
                
                # R√©cup√©rer l'op√©rateur et l'essai
                if col_idx - 1 < len(column_info):
                    op_str, trial_str = column_info[col_idx]
                else:
                    # Fallback si les indices ne correspondent pas
                    op_num = ((col_idx - 1) // 3) + 1
                    trial_num = ((col_idx - 1) % 3) + 1
                    op_str = f"O{op_num}"
                    trial_str = str(trial_num)
                
                long_data.append({
                    "Pi√®ce": f"P{piece_str}",
                    "Op√©rateur": op_str,
                    "Essai": int(trial_str),
                    "Mesure": float(measure_val)
                })
    
    result_df = pd.DataFrame(long_data)
    
    if result_df.empty:
        raise ValueError("Aucune donn√©e valide n'a √©t√© extraite du fichier")
    
    # Trier pour avoir un ordre coh√©rent
    result_df = result_df.sort_values(["Pi√®ce", "Op√©rateur", "Essai"])
    
    return result_df


def detect_and_convert_format(df: pd.DataFrame) -> pd.DataFrame:
    """
    D√©tecte automatiquement le format et convertit en format long si n√©cessaire.
    """
    # Si d√©j√† en format long (4 colonnes avec les bons noms)
    if len(df.columns) == 4:
        required_cols = {"Pi√®ce", "Op√©rateur", "Essai", "Mesure"}
        if required_cols.issubset(set(df.columns)):
            return normalize_imported_columns(df)
    
    # Essayer de d√©tecter le format sp√©cifique du template
    # V√©rifier si le fichier a la structure du template
    
    # M√©thode 1: V√©rifier les noms de colonnes
    col_names_str = str(df.columns.tolist()).upper()
    
    # M√©thode 2: V√©rifier les premi√®res lignes
    has_operateur_in_first_row = False
    has_essai_in_second_row = False
    
    if len(df) >= 2:
        # V√©rifier la premi√®re ligne pour "OPERATEUR"
        first_row_vals = df.iloc[0].astype(str).str.upper().tolist()
        has_operateur_in_first_row = any("OPERATEUR" in val or "OPERATOR" in val for val in first_row_vals)
        
        # V√©rifier la deuxi√®me ligne pour "ESSAI" ou des nombres
        second_row_vals = df.iloc[1].astype(str).str.upper().tolist()
        has_essai_in_second_row = any("ESSAI" in val or "TRIAL" in val or val.strip().isdigit() for val in second_row_vals[1:])
    
    # M√©thode 3: V√©rifier le MultiIndex
    is_multiindex = isinstance(df.columns, pd.MultiIndex)
    
    # D√©cision: si l'une de ces conditions est vraie, c'est probablement le format template
    if (is_multiindex or 
        has_operateur_in_first_row or 
        has_essai_in_second_row or
        "OPERATEUR" in col_names_str):
        
        try:
            return convert_wide_to_long_specific(df)
        except Exception as e:
            st.warning(f"√âchec de la conversion sp√©cifique: {e}. Tentative de conversion g√©n√©rique...")
    
    # Sinon, essayer de convertir du format large g√©n√©rique
    try:
        return convert_wide_to_long_generic(df)
    except Exception as e:
        st.error(f"Erreur lors de la conversion du format: {str(e)}")
        
        # Afficher un aper√ßu du DataFrame pour aider au d√©bogage
        st.write("**Aper√ßu du DataFrame brut:**")
        st.dataframe(df.head())
        st.write("**Types de donn√©es:**")
        st.write(df.dtypes)
        
        # Essayer de normaliser les colonnes
        return normalize_imported_columns(df)


def convert_wide_to_long_generic(df: pd.DataFrame) -> pd.DataFrame:
    """
    Conversion g√©n√©rique de format large √† long.
    """
    # Si le DataFrame a un MultiIndex (header=[0,1])
    if isinstance(df.columns, pd.MultiIndex):
        # Reconstruire en DataFrame simple
        data = df.values
        new_columns = []
        
        for col in df.columns:
            if len(col) >= 2 and not pd.isna(col[0]) and not pd.isna(col[1]):
                new_columns.append(f"{col[0]}_{col[1]}")
            elif len(col) >= 1 and not pd.isna(col[0]):
                new_columns.append(str(col[0]))
            else:
                new_columns.append(f"Col{len(new_columns)}")
        
        df = pd.DataFrame(data, columns=new_columns)
    
    # Essayer de deviner la structure
    # Si la premi√®re colonne semble √™tre des identifiants de pi√®ces
    if len(df.columns) > 0:
        first_col_name = str(df.columns[0]).lower()
        if "pi√®ce" in first_col_name or "part" in first_col_name or "n¬∞" in first_col_name:
            # Cela pourrait √™tre la colonne des pi√®ces
            df_long = df.melt(id_vars=[df.columns[0]], var_name="Op√©rateur_Essai", value_name="Mesure")
            df_long = df_long.rename(columns={df.columns[0]: "Pi√®ce"})
            
            # Essayer d'extraire op√©rateur et essai du nom de colonne
            def split_op_trial(col_name):
                col_str = str(col_name)
                # Chercher des motifs courants
                for sep in ['_', ' ', '-', '.']:
                    if sep in col_str:
                        parts = col_str.split(sep)
                        if len(parts) >= 2:
                            return parts[0], parts[1]
                
                # Si pas de s√©parateur, chercher des motifs communs
                match_op = re.search(r'([A-Za-z]+)\s*(\d+)', col_str)
                if match_op:
                    return match_op.group(1), match_op.group(2)
                
                # Fallback
                return "O1", "1"
            
            df_long[["Op√©rateur", "Essai"]] = df_long["Op√©rateur_Essai"].apply(
                lambda x: pd.Series(split_op_trial(x))
            )
            df_long = df_long.drop(columns=["Op√©rateur_Essai"])
            
            # Convertir les types
            df_long["Essai"] = pd.to_numeric(df_long["Essai"], errors='coerce')
            df_long["Mesure"] = pd.to_numeric(df_long["Mesure"], errors='coerce')
            
            return df_long[["Pi√®ce", "Op√©rateur", "Essai", "Mesure"]].dropna()
    
    raise ValueError("Format non reconnu - impossible de convertir en format long")


def validate_dataset(df: pd.DataFrame, n_parts: int, n_ops: int, n_trials: int) -> Tuple[bool, List[str]]:
    errors: List[str] = []
    required = {"Pi√®ce", "Op√©rateur", "Essai", "Mesure"}
    missing = required - set(df.columns)
    if missing:
        errors.append(f"Colonnes manquantes: {', '.join(sorted(missing))}.")
        return False, errors

    df2 = df.copy()
    df2["Pi√®ce"] = df2["Pi√®ce"].astype(str).str.strip()
    df2["Op√©rateur"] = df2["Op√©rateur"].astype(str).str.strip()
    df2["Essai"] = pd.to_numeric(df2["Essai"], errors="coerce")
    df2["Mesure"] = pd.to_numeric(df2["Mesure"], errors="coerce")

    if df2["Essai"].isna().any():
        errors.append("La colonne 'Essai' contient des valeurs non num√©riques.")
    if df2["Mesure"].isna().any():
        errors.append("La colonne 'Mesure' contient des valeurs vides ou non num√©riques.")

    parts = sorted(df2["Pi√®ce"].unique())
    ops = sorted(df2["Op√©rateur"].unique())

    if len(parts) != n_parts:
        errors.append(f"Nombre de pi√®ces d√©tect√©es = {len(parts)} (attendu = {n_parts}).")
    if len(ops) != n_ops:
        errors.append(f"Nombre d'op√©rateurs d√©tect√© = {len(ops)} (attendu = {n_ops}).")

    # V√©rification d√©taill√©e de l'√©quilibre du plan
    counts = df2.groupby(["Pi√®ce", "Op√©rateur"])["Mesure"].count()
    
    if counts.nunique() != 1:
        # Afficher les d√©tails des in√©galit√©s
        unique_counts = counts.unique()
        errors.append(f"Plan non √©quilibr√©: le nombre de mesures varie selon les couples Pi√®ce x Op√©rateur.")
        
        # Ajouter des d√©tails suppl√©mentaires
        min_count = counts.min()
        max_count = counts.max()
        avg_count = counts.mean()
        
        errors.append(f"  - Nombre minimum de mesures par couple: {min_count}")
        errors.append(f"  - Nombre maximum de mesures par couple: {max_count}")
        errors.append(f"  - Nombre moyen de mesures par couple: {avg_count:.2f}")
        
        # Trouver les couples probl√©matiques
        problem_couples = counts[counts != n_trials].index.tolist()
        if problem_couples:
            errors.append(f"  - Couples probl√©matiques (‚â† {n_trials} mesures): {len(problem_couples)}")
            if len(problem_couples) <= 10:  # Afficher seulement les 10 premiers
                for couple in problem_couples[:10]:
                    errors.append(f"    * {couple[0]}, {couple[1]}: {counts[couple]} mesures")
    else:
        r = int(counts.iloc[0])
        if r != n_trials:
            errors.append(f"Nombre de r√©p√©titions d√©tect√© = {r} (attendu = {n_trials}).")

    expected_rows = n_parts * n_ops * n_trials
    if len(df2) != expected_rows:
        errors.append(f"Nombre de lignes = {len(df2)} (attendu = {expected_rows} = pi√®ces√óop√©rateurs√óessais).")

    return (len(errors) == 0), errors


# -----------------------------
# Calcul Gage R&R (ANOVA) avec tests F
# -----------------------------
def gage_rr_anova(df: pd.DataFrame, confidence_level: float = 0.95) -> AnovaResult:
    """
    Calcule l'√©tude Gage R&R par ANOVA (plan √©quilibr√©) avec tests d'hypoth√®se.
    
    DF attendu: colonnes Pi√®ce, Op√©rateur, Essai, Mesure
    confidence_level: niveau de confiance (ex: 0.95 pour 95%)
    """
    d = df.copy()
    d["Pi√®ce"] = d["Pi√®ce"].astype(str)
    d["Op√©rateur"] = d["Op√©rateur"].astype(str)
    d["Essai"] = d["Essai"].astype(int)
    d["Mesure"] = pd.to_numeric(d["Mesure"], errors="coerce")

    if d["Mesure"].isna().any():
        raise ValueError("Valeurs de mesure manquantes / non num√©riques.")

    parts = sorted(d["Pi√®ce"].unique())
    ops = sorted(d["Op√©rateur"].unique())
    p = len(parts)
    o = len(ops)

    counts = d.groupby(["Pi√®ce", "Op√©rateur"])["Mesure"].count()
    if counts.nunique() != 1:
        raise ValueError("Plan non √©quilibr√©: r√©p√©titions diff√©rentes selon Pi√®ce x Op√©rateur.")
    r = int(counts.iloc[0])

    grand_mean = d["Mesure"].mean()
    mean_part = d.groupby("Pi√®ce")["Mesure"].mean()
    mean_op = d.groupby("Op√©rateur")["Mesure"].mean()
    mean_po = d.groupby(["Pi√®ce", "Op√©rateur"])["Mesure"].mean()

    # Sommes des carr√©s (balanced ANOVA)
    SS_part = o * r * ((mean_part - grand_mean) ** 2).sum()
    SS_op = p * r * ((mean_op - grand_mean) ** 2).sum()

    tmp = mean_po.reset_index().rename(columns={"Mesure": "mean_po"})
    tmp["mean_part"] = tmp["Pi√®ce"].map(mean_part)
    tmp["mean_op"] = tmp["Op√©rateur"].map(mean_op)
    tmp["res"] = tmp["mean_po"] - tmp["mean_part"] - tmp["mean_op"] + grand_mean
    SS_po = r * (tmp["res"] ** 2).sum()

    d2 = d.merge(mean_po.rename("mean_po").reset_index(), on=["Pi√®ce", "Op√©rateur"], how="left")
    SS_e = ((d2["Mesure"] - d2["mean_po"]) ** 2).sum()
    SS_total = ((d["Mesure"] - grand_mean) ** 2).sum()

    # ddl
    df_part = p - 1
    df_op = o - 1
    df_po = (p - 1) * (o - 1)
    df_e = p * o * (r - 1)
    df_total = p * o * r - 1

    # Carr√©s moyens
    MS_part = SS_part / df_part if df_part > 0 else np.nan
    MS_op = SS_op / df_op if df_op > 0 else np.nan
    MS_po = SS_po / df_po if df_po > 0 else np.nan
    MS_e = SS_e / df_e if df_e > 0 else np.nan

    # Tests d'hypoth√®se F (tests statistiques)
    alpha = 1 - confidence_level
    
    # Test pour l'interaction Pi√®ce*Op√©rateur
    if df_po > 0 and df_e > 0:
        F_po = MS_po / MS_e if MS_e > 0 else np.nan
        p_value_po = 1 - stats.f.cdf(F_po, df_po, df_e) if not np.isnan(F_po) else np.nan
        F_crit_po = stats.f.ppf(1 - alpha, df_po, df_e) if df_po > 0 and df_e > 0 else np.nan
    else:
        F_po, p_value_po, F_crit_po = np.nan, np.nan, np.nan
    
    # Test pour l'op√©rateur (si interaction non significative ou si on utilise MS_e)
    if df_po > 0:
        F_op = MS_op / MS_po if MS_po > 0 else np.nan
        p_value_op = 1 - stats.f.cdf(F_op, df_op, df_po) if not np.isnan(F_op) else np.nan
        F_crit_op = stats.f.ppf(1 - alpha, df_op, df_po) if df_op > 0 and df_po > 0 else np.nan
    else:
        F_op = MS_op / MS_e if MS_e > 0 else np.nan
        p_value_op = 1 - stats.f.cdf(F_op, df_op, df_e) if not np.isnan(F_op) else np.nan
        F_crit_op = stats.f.ppf(1 - alpha, df_op, df_e) if df_op > 0 and df_e > 0 else np.nan
    
    # Test pour la pi√®ce
    if df_po > 0:
        F_part = MS_part / MS_po if MS_po > 0 else np.nan
        p_value_part = 1 - stats.f.cdf(F_part, df_part, df_po) if not np.isnan(F_part) else np.nan
        F_crit_part = stats.f.ppf(1 - alpha, df_part, df_po) if df_part > 0 and df_po > 0 else np.nan
    else:
        F_part = MS_part / MS_e if MS_e > 0 else np.nan
        p_value_part = 1 - stats.f.cdf(F_part, df_part, df_e) if not np.isnan(F_part) else np.nan
        F_crit_part = stats.f.ppf(1 - alpha, df_part, df_e) if df_part > 0 and df_e > 0 else np.nan

    # Composantes de variance (m√©thode ANOVA)
    sigma_e2 = MS_e  # r√©p√©tabilit√© (Equipment)

    # Si l'interaction est significative (p < alpha), on l'inclut
    include_interaction = p_value_po < alpha if not np.isnan(p_value_po) else False
    
    if include_interaction:
        sigma_po2 = max((MS_po - MS_e) / r, 0.0) if df_po > 0 else 0.0  # interaction
        sigma_o2 = max((MS_op - MS_po) / (p * r), 0.0) if df_po > 0 else 0.0  # op√©rateur
        sigma_p2 = max((MS_part - MS_po) / (o * r), 0.0) if df_po > 0 else 0.0  # pi√®ce-√†-pi√®ce
    else:
        # Si interaction non significative, on la combine avec l'erreur
        sigma_po2 = 0.0
        sigma_o2 = max((MS_op - MS_e) / (p * r), 0.0)
        sigma_p2 = max((MS_part - MS_e) / (o * r), 0.0)

    sigma_grr2 = sigma_e2 + sigma_o2 + sigma_po2
    sigma_total2 = sigma_grr2 + sigma_p2

    # Sigmas
    sigma_e = math.sqrt(max(sigma_e2, 0.0))
    sigma_av = math.sqrt(max(sigma_o2 + sigma_po2, 0.0))
    sigma_p = math.sqrt(max(sigma_p2, 0.0))
    sigma_grr = math.sqrt(max(sigma_grr2, 0.0))
    sigma_total = math.sqrt(max(sigma_total2, 0.0))

    # Study Variation (6œÉ)
    EV = 6.0 * sigma_e
    AV = 6.0 * sigma_av
    GRR = 6.0 * sigma_grr
    PV = 6.0 * sigma_p
    TV = 6.0 * sigma_total

    pct_EV = (EV / TV) * 100 if TV > 0 else np.nan
    pct_AV = (AV / TV) * 100 if TV > 0 else np.nan
    pct_GRR = (GRR / TV) * 100 if TV > 0 else np.nan
    pct_PV = (PV / TV) * 100 if TV > 0 else np.nan

    # %Contribution (variance)
    contrib = {
        "√âquipement (EV)": sigma_e2,
        "Op√©rateur": sigma_o2,
        "Pi√®ce*Op√©rateur": sigma_po2,
        "Pi√®ce-√†-pi√®ce (PV)": sigma_p2,
        "Total": sigma_total2,
    }
    pct_contrib = {k: (v / sigma_total2) * 100 if sigma_total2 > 0 else np.nan for k, v in contrib.items()}

    ndc = int(math.floor(1.41 * (PV / GRR))) if GRR > 0 else 0

    # Table ANOVA avec p-values et tests F
    anova_table = pd.DataFrame(
        {
            "Source": ["Pi√®ce", "Op√©rateur", "Pi√®ce*Op√©rateur", "Erreur (R√©p√©tabilit√©)", "Total"],
            "SS": [SS_part, SS_op, SS_po, SS_e, SS_total],
            "ddl": [df_part, df_op, df_po, df_e, df_total],
            "MS": [MS_part, MS_op, MS_po, MS_e, np.nan],
            "F": [F_part, F_op, F_po, np.nan, np.nan],
            "p-value": [p_value_part, p_value_op, p_value_po, np.nan, np.nan],
            f"F-crit ({int(confidence_level*100)}%)": [F_crit_part, F_crit_op, F_crit_po, np.nan, np.nan],
        }
    )

    var_components = pd.DataFrame(
        {
            "Composante": [
                "EV (R√©p√©tabilit√©)",
                "Op√©rateur",
                "Pi√®ce*Op√©rateur",
                "AV (Reproductibilit√©)",
                "PV (Vp)",
                "GRR",
                "TV (Vt)",
            ],
            "Variance": [sigma_e2, sigma_o2, sigma_po2, sigma_o2 + sigma_po2, sigma_p2, sigma_grr2, sigma_total2],
            "Sigma": [
                sigma_e,
                math.sqrt(max(sigma_o2, 0.0)),
                math.sqrt(max(sigma_po2, 0.0)),
                sigma_av,
                sigma_p,
                sigma_grr,
                sigma_total,
            ],
        }
    )

    var_components["%Contribution"] = np.nan
    base_map = {
        "EV (R√©p√©tabilit√©)": "√âquipement (EV)",
        "Op√©rateur": "Op√©rateur",
        "Pi√®ce*Op√©rateur": "Pi√®ce*Op√©rateur",
        "PV (Vp)": "Pi√®ce-√†-pi√®ce (PV)",
        "TV (Vt)": "Total",
    }
    for i, row in var_components.iterrows():
        label = row["Composante"]
        if label in base_map:
            var_components.loc[i, "%Contribution"] = pct_contrib[base_map[label]]

    study_var = pd.DataFrame(
        {
            "Indicateur": ["EV", "AV", "GRR", "PV (Vp)", "TV (Vt)"],
            "Study Variation (6œÉ)": [EV, AV, GRR, PV, TV],
            "%StudyVar": [pct_EV, pct_AV, pct_GRR, pct_PV, 100.0],
        }
    )

    # Tests F stock√©s
    f_tests = {
        "Pi√®ce": (F_part, p_value_part, F_crit_part),
        "Op√©rateur": (F_op, p_value_op, F_crit_op),
        "Pi√®ce*Op√©rateur": (F_po, p_value_po, F_crit_po),
    }

    # D√©cision sur l'inclusion de l'interaction
    interaction_msg = "incluse" if include_interaction else "exclue (non significative)"
    interaction_decision = f"Interaction Pi√®ce*Op√©rateur {interaction_msg} (p-value = {p_value_po:.4f}, Œ± = {alpha:.3f})"

    # Interpr√©tation (selon %R&R = %StudyVar de GRR)
    if pct_GRR < 10:
        niveau = "Satisfaisant"
        regle = "< 10 % : le processus est satisfaisant"
        css = "rr-green"
    elif pct_GRR <= 30:
        niveau = "Acceptable"
        regle = "Entre 10 % et 30 % : acceptable mais am√©liorable"
        css = "rr-orange"
    else:
        niveau = "Inacceptable"
        regle = "> 30 % : le processus est inacceptable"
        css = "rr-red"

    conclusion = {
        "niveau": niveau,
        "regle": regle,
        "css": css,
        "pct_grr": f"{pct_GRR:.2f}%",
        "ndc": str(ndc),
        "interaction_decision": interaction_decision,
        "include_interaction": include_interaction,
        "confidence_level": f"{confidence_level*100:.1f}%"
    }

    metrics = {
        "EV": EV,
        "AV": AV,
        "PV": PV,
        "TV": TV,
        "GRR": GRR,
        "%GRR": pct_GRR,
        "%EV": pct_EV,
        "%AV": pct_AV,
        "%PV": pct_PV,
        "ndc": ndc,
        "confidence_level": confidence_level,
    }

    return AnovaResult(
        p=p,
        o=o,
        r=r,
        confidence_level=confidence_level,
        anova_table=anova_table,
        var_components=var_components,
        study_var=study_var,
        metrics=metrics,
        conclusion=conclusion,
        f_tests=f_tests,
    )


# -----------------------------
# Graphiques (matplotlib)
# -----------------------------
def fig_boxplot(df: pd.DataFrame, by: str, title: str) -> plt.Figure:
    fig, ax = plt.subplots()
    df.boxplot(column="Mesure", by=by, ax=ax, grid=False)
    ax.set_title(title)
    ax.set_xlabel(by)
    ax.set_ylabel("Mesure")
    plt.suptitle("")
    fig.tight_layout()
    return fig


def fig_interaction(df: pd.DataFrame) -> plt.Figure:
    means = df.groupby(["Pi√®ce", "Op√©rateur"])["Mesure"].mean().reset_index()
    pivot = means.pivot(index="Pi√®ce", columns="Op√©rateur", values="Mesure")

    fig, ax = plt.subplots()
    x = np.arange(len(pivot.index))
    for op in pivot.columns:
        ax.plot(x, pivot[op].values, marker="o", label=str(op))

    ax.set_xticks(x)
    ax.set_xticklabels(pivot.index.tolist(), rotation=45, ha="right")
    ax.set_title("Graphique d'interaction (moyennes)")
    ax.set_xlabel("Pi√®ce")
    ax.set_ylabel("Mesure moyenne")
    ax.legend(title="Op√©rateur", bbox_to_anchor=(1.02, 1), loc="upper left")
    fig.tight_layout()
    return fig


def fig_contribution(var_components: pd.DataFrame) -> plt.Figure:
    base = var_components.dropna(subset=["%Contribution"]).copy()
    order = ["EV (R√©p√©tabilit√©)", "Op√©rateur", "Pi√®ce*Op√©rateur", "PV (Vp)", "TV (Vt)"]
    base = base[base["Composante"].isin(order)]
    base["Composante"] = pd.Categorical(base["Composante"], categories=order, ordered=True)
    base = base.sort_values("Composante")

    fig, ax = plt.subplots()
    ax.bar(base["Composante"].astype(str), base["%Contribution"].values)
    ax.set_title("%Contribution (variance)")
    ax.set_ylabel("%")
    ax.set_xlabel("Composante")
    ax.set_xticks(np.arange(len(base)))
    ax.set_xticklabels(base["Composante"].astype(str).tolist(), rotation=30, ha="right")
    fig.tight_layout()
    return fig


def fig_studyvar(study_var: pd.DataFrame) -> plt.Figure:
    fig, ax = plt.subplots()
    ax.bar(study_var["Indicateur"], study_var["%StudyVar"].values)
    ax.set_title("%Study Variation (6œÉ)")
    ax.set_ylabel("%")
    ax.set_xlabel("Indicateur")
    fig.tight_layout()
    return fig


def fig_pvalues(anova_table: pd.DataFrame) -> plt.Figure:
    """Graphique des p-values pour visualiser la signification statistique"""
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # Filtrer les sources avec p-values
    sources = []
    p_values = []
    
    for idx, row in anova_table.iterrows():
        if not pd.isna(row['p-value']):
            sources.append(row['Source'])
            p_values.append(row['p-value'])
    
    x = np.arange(len(sources))
    bars = ax.bar(x, p_values, color='skyblue', alpha=0.7)
    
    # Ligne √† alpha = 0.05
    ax.axhline(y=0.05, color='red', linestyle='--', linewidth=2, label='Œ± = 0.05')
    
    # Colorer les barres selon la signification
    for i, (bar, p_val) in enumerate(zip(bars, p_values)):
        if p_val < 0.05:
            bar.set_color('lightcoral')
            bar.set_alpha(0.9)
    
    ax.set_xlabel('Source de variation')
    ax.set_ylabel('p-value')
    ax.set_title('Tests d\'hypoth√®se - p-values')
    ax.set_xticks(x)
    ax.set_xticklabels(sources, rotation=45, ha='right')
    ax.set_ylim([0, max(p_values) * 1.2 if p_values else 1])
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    fig.tight_layout()
    return fig


# -----------------------------
# Rapport PDF (ReportLab)
# -----------------------------
def fig_to_rl_image(fig: plt.Figure, width_cm: float = 16.5) -> RLImage:
    buf = io.BytesIO()
    fig.savefig(buf, format="png", dpi=220, bbox_inches="tight")
    plt.close(fig)
    buf.seek(0)

    img_reader = ImageReader(buf)
    iw, ih = img_reader.getSize()
    w = width_cm * cm
    h = w * (ih / iw) if iw else 10 * cm
    return RLImage(buf, width=w, height=h)


def df_to_rl_table(df: pd.DataFrame, col_widths: Optional[List[float]] = None, max_rows: int = 30) -> Table:
    df2 = df.copy()
    if len(df2) > max_rows:
        df2 = df2.head(max_rows)

    data = [df2.columns.tolist()] + df2.values.tolist()
    t = Table(data, colWidths=col_widths)
    t.setStyle(
        TableStyle(
            [
                ("BACKGROUND", (0, 0), (-1, 0), colors.HexColor("#f3f4f6")),
                ("TEXTCOLOR", (0, 0), (-1, 0), colors.HexColor("#111827")),
                ("GRID", (0, 0), (-1, -1), 0.4, colors.HexColor("#d1d5db")),
                ("FONTNAME", (0, 0), (-1, 0), "Helvetica-Bold"),
                ("FONTSIZE", (0, 0), (-1, -1), 8.5),
                ("VALIGN", (0, 0), (-1, -1), "MIDDLE"),
                ("ALIGN", (1, 1), (-1, -1), "RIGHT"),
                ("LEFTPADDING", (0, 0), (-1, -1), 6),
                ("RIGHTPADDING", (0, 0), (-1, -1), 6),
                ("TOPPADDING", (0, 0), (-1, -1), 3),
                ("BOTTOMPADDING", (0, 0), (-1, -1), 3),
            ]
        )
    )
    return t


def generate_pdf_report(df: pd.DataFrame, res: AnovaResult, tol: Optional[float] = None) -> bytes:
    buffer = io.BytesIO()
    doc = SimpleDocTemplate(
        buffer,
        pagesize=A4,
        rightMargin=1.7 * cm,
        leftMargin=1.7 * cm,
        topMargin=1.6 * cm,
        bottomMargin=1.6 * cm,
        title="Rapport Gage R&R",
        author="Streamlit App",
    )
    styles = getSampleStyleSheet()

    story = []
    story.append(Paragraph("Rapport d'√©tude Gage R&amp;R (Cage R&amp;R)", styles["Title"]))
    story.append(Paragraph(f"G√©n√©r√© le {datetime.now().strftime('%Y-%m-%d %H:%M')}", styles["Normal"]))
    story.append(Spacer(1, 0.4 * cm))

    story.append(Paragraph("1) Param√®tres", styles["Heading2"]))
    story.append(
        Paragraph(
            f"- Pi√®ces: <b>{res.p}</b> &nbsp;&nbsp;|&nbsp;&nbsp; "
            f"Op√©rateurs: <b>{res.o}</b> &nbsp;&nbsp;|&nbsp;&nbsp; "
            f"R√©p√©titions (essais): <b>{res.r}</b> &nbsp;&nbsp;|&nbsp;&nbsp; "
            f"Niveau de confiance: <b>{res.conclusion['confidence_level']}</b>",
            styles["Normal"],
        )
    )
    story.append(Paragraph(f"- <b>{res.conclusion['interaction_decision']}</b>", styles["Normal"]))
    
    if tol and tol > 0:
        pct_tol = (res.metrics["GRR"] / tol) * 100
        story.append(
            Paragraph(
                f"- Tol√©rance (optionnelle): <b>{tol}</b> ‚Üí %Tol√©rance(GRR) ‚âà <b>{pct_tol:.2f}%</b>",
                styles["Normal"],
            )
        )
    story.append(Spacer(1, 0.35 * cm))

    story.append(Paragraph("2) Conclusion", styles["Heading2"]))
    story.append(
        Paragraph(
            f"<b>%R&amp;R (Study Variation) = {res.metrics['%GRR']:.2f}%</b> ‚Äî "
            f"Niveau: <b>{res.conclusion['niveau']}</b> ‚Äî "
            f"ndc: <b>{res.metrics['ndc']}</b><br/>"
            f"R√®gle: {res.conclusion['regle']}",
            styles["Normal"],
        )
    )
    story.append(Spacer(1, 0.4 * cm))

    story.append(Paragraph("3) R√©sultats principaux", styles["Heading2"]))
    synth = pd.DataFrame(
        {
            "Indicateur": ["EV", "AV", "GRR", "PV (Vp)", "TV (Vt)", "%GRR", "ndc"],
            "Valeur": [
                _fmt(res.metrics["EV"]),
                _fmt(res.metrics["AV"]),
                _fmt(res.metrics["GRR"]),
                _fmt(res.metrics["PV"]),
                _fmt(res.metrics["TV"]),
                f"{res.metrics['%GRR']:.2f}%",
                str(res.metrics["ndc"]),
            ],
        }
    )
    story.append(df_to_rl_table(synth, col_widths=[5.0 * cm, 10.0 * cm], max_rows=20))
    story.append(Spacer(1, 0.5 * cm))

    story.append(Paragraph("4) Table ANOVA", styles["Heading2"]))
    anova_df = res.anova_table.copy()
    story.append(df_to_rl_table(anova_df.round({"SS": 6, "MS": 6, "F": 4, "p-value": 4}), max_rows=20))
    story.append(Spacer(1, 0.45 * cm))

    story.append(Paragraph("5) Composantes de variance", styles["Heading2"]))
    vc = res.var_components.copy()
    story.append(df_to_rl_table(vc.round({"Variance": 6, "Sigma": 6, "%Contribution": 2}), max_rows=30))
    story.append(Spacer(1, 0.45 * cm))

    story.append(Paragraph("6) %Study Variation (6œÉ)", styles["Heading2"]))
    sv = res.study_var.copy()
    story.append(df_to_rl_table(sv.round({"Study Variation (6œÉ)": 6, "%StudyVar": 2}), max_rows=20))

    story.append(PageBreak())

    story.append(Paragraph("7) Graphes", styles["Heading2"]))
    story.append(Spacer(1, 0.15 * cm))

    story.append(fig_to_rl_image(fig_boxplot(df, by="Op√©rateur", title="Boxplot des mesures par op√©rateur"), width_cm=17.2))
    story.append(Spacer(1, 0.35 * cm))
    story.append(fig_to_rl_image(fig_boxplot(df, by="Pi√®ce", title="Boxplot des mesures par pi√®ce"), width_cm=17.2))
    story.append(Spacer(1, 0.35 * cm))
    story.append(fig_to_rl_image(fig_interaction(df), width_cm=17.2))
    story.append(Spacer(1, 0.35 * cm))
    story.append(fig_to_rl_image(fig_studyvar(res.study_var), width_cm=17.2))
    story.append(Spacer(1, 0.35 * cm))
    story.append(fig_to_rl_image(fig_contribution(res.var_components), width_cm=17.2))
    story.append(Spacer(1, 0.35 * cm))
    story.append(fig_to_rl_image(fig_pvalues(res.anova_table), width_cm=17.2))

    story.append(Spacer(1, 0.35 * cm))
    story.append(
        Paragraph(
            f"<span fontSize=8 color='#6b7280'>Note: Calculs bas√©s sur une √©tude ANOVA √† plan √©quilibr√©. "
            f"Study Variation = 6œÉ. Niveau de confiance utilis√©: {res.conclusion['confidence_level']}.</span>",
            styles["Normal"],
        )
    )

    doc.build(story)
    return buffer.getvalue()


def export_excel(df: pd.DataFrame, res: AnovaResult) -> bytes:
    out = io.BytesIO()
    with pd.ExcelWriter(out, engine="openpyxl") as writer:
        df.to_excel(writer, sheet_name="Donnees", index=False)
        res.study_var.to_excel(writer, sheet_name="StudyVar", index=False)
        res.var_components.to_excel(writer, sheet_name="Variance", index=False)
        res.anova_table.to_excel(writer, sheet_name="ANOVA", index=False)
        pd.DataFrame([res.metrics]).to_excel(writer, sheet_name="KPIs", index=False)
        
        # Ajouter les tests F
        f_tests_df = pd.DataFrame([
            {"Source": "Pi√®ce", "F": res.f_tests["Pi√®ce"][0], "p-value": res.f_tests["Pi√®ce"][1], f"F-crit ({res.confidence_level*100:.0f}%)": res.f_tests["Pi√®ce"][2]},
            {"Source": "Op√©rateur", "F": res.f_tests["Op√©rateur"][0], "p-value": res.f_tests["Op√©rateur"][1], f"F-crit ({res.confidence_level*100:.0f}%)": res.f_tests["Op√©rateur"][2]},
            {"Source": "Pi√®ce*Op√©rateur", "F": res.f_tests["Pi√®ce*Op√©rateur"][0], "p-value": res.f_tests["Pi√®ce*Op√©rateur"][1], f"F-crit ({res.confidence_level*100:.0f}%)": res.f_tests["Pi√®ce*Op√©rateur"][2]},
        ])
        f_tests_df.to_excel(writer, sheet_name="Tests_F", index=False)
        
    return out.getvalue()


# -----------------------------
# UI Streamlit
# -----------------------------
st.title("üìè Calculateur Gage R&R (Cage R&R) ‚Äî ANOVA")
st.caption("Saisie manuelle ou import (CSV/Excel) ‚Üí calcul EV, AV, Vp, Vt + interpr√©tation + rapport PDF.")

with st.sidebar:
    st.header("Param√®tres")
    n_parts = st.number_input("Nombre de pi√®ces", min_value=2, max_value=50, value=10, step=1)
    n_ops = st.number_input("Nombre d'op√©rateurs", min_value=2, max_value=20, value=3, step=1)
    n_trials = st.number_input("Nombre de mesures (essais) par op√©rateur & pi√®ce", min_value=2, max_value=10, value=3, step=1)

    st.divider()
    entry_mode = st.radio("Mode de saisie des donn√©es", ["Saisie manuelle", "Importer (CSV/Excel)"], horizontal=False)

    st.divider()
    st.subheader("Param√®tres statistiques")
    confidence_level = st.selectbox(
        "Niveau de confiance",
        options=[0.90, 0.95, 0.99],
        format_func=lambda x: f"{x*100:.0f}%",
        index=1
    )
    
    st.divider()
    tol = st.number_input("Tol√©rance (optionnel, pour %Tol√©rance)", min_value=0.0, value=0.0, step=0.1)

    st.divider()
    st.markdown(
        "<div class='small-note'>üí° <b>Formats accept√©s</b>:<br>"
        "- <b>Format long</b>: colonnes Pi√®ce, Op√©rateur, Essai, Mesure<br>"
        "- <b>Format large (template)**: comme TEMPLATE CAGE RR.xlsx (2 lignes d'en-t√™te)</div>",
        unsafe_allow_html=True,
    )

tabs = st.tabs(["1) Donn√©es", "2) R√©sultats", "3) Graphes", "4) Rapport / Export"])

# --- Tab 1 : Donn√©es
with tabs[0]:
    st.subheader("Donn√©es d'entr√©e")

    shape = (int(n_parts), int(n_ops), int(n_trials))
    if "template_shape" not in st.session_state or st.session_state["template_shape"] != shape:
        st.session_state["template_shape"] = shape
        st.session_state["df_data"] = build_template(*shape)

    if entry_mode == "Saisie manuelle":
        st.markdown("Remplissez la colonne **Mesure** (valeurs num√©riques).")
        edited = st.data_editor(
            st.session_state["df_data"],
            use_container_width=True,
            num_rows="fixed",
            hide_index=True,
            key="editor_manual",
        )
        st.session_state["df_data"] = edited

        template_csv = build_template(*shape).to_csv(index=False).encode("utf-8")
        st.download_button("‚¨áÔ∏è T√©l√©charger un template CSV", data=template_csv, file_name="template_gage_rr.csv", mime="text/csv")

    else:
        st.markdown("Importez un fichier **CSV** ou **Excel**.")
        st.markdown("**Formats accept√©s:**")
        st.markdown("- **Format long**: colonnes Pi√®ce, Op√©rateur, Essai, Mesure")
        st.markdown("- **Format large (template)**: comme TEMPLATE CAGE RR.xlsx (2 lignes d'en-t√™te)")
        
        upl = st.file_uploader("Importer", type=["csv", "xlsx"])

        if upl is not None:
            try:
                # Lire le fichier
                if upl.name.lower().endswith(".csv"):
                    # Essayer plusieurs m√©thodes pour lire le CSV
                    try:
                        # Essayer avec header=[0,1] pour les templates
                        df_imp = pd.read_csv(upl, header=[0, 1], encoding='utf-8')
                    except:
                        try:
                            # Essayer avec header=None
                            df_imp = pd.read_csv(upl, header=None, encoding='utf-8')
                        except:
                            # Dernier essai: lecture standard
                            df_imp = pd.read_csv(upl, encoding='utf-8')
                else:
                    # Pour Excel
                    try:
                        # Essayer avec header=[0,1] pour les templates
                        df_imp = pd.read_excel(upl, header=[0, 1])
                    except:
                        try:
                            # Essayer avec header=None
                            df_imp = pd.read_excel(upl, header=None)
                        except:
                            # Dernier essai: lecture standard
                            df_imp = pd.read_excel(upl)
                
                # Afficher les d√©tails du fichier brut
                with st.expander("üìä D√©tails du fichier import√©"):
                    st.write(f"**Dimensions:** {df_imp.shape[0]} lignes √ó {df_imp.shape[1]} colonnes")
                    st.write("**Aper√ßu des 5 premi√®res lignes:**")
                    st.dataframe(df_imp.head(), use_container_width=True)
                
                # Essayer de convertir en format long
                try:
                    # D√©tecter et convertir le format
                    df_converted = detect_and_convert_format(df_imp)
                    
                    if df_converted.empty:
                        st.error("‚ö†Ô∏è La conversion n'a produit aucune donn√©e valide.")
                    else:
                        st.success(f"‚úÖ Conversion r√©ussie ! {len(df_converted)} lignes extraites.")
                        
                        # Afficher un aper√ßu des donn√©es converties
                        st.markdown("**Aper√ßu des donn√©es converties :**")
                        st.dataframe(df_converted.head(15), use_container_width=True)
                        
                        # Calculer les statistiques de base
                        unique_parts = df_converted["Pi√®ce"].nunique()
                        unique_ops = df_converted["Op√©rateur"].nunique()
                        unique_trials = df_converted["Essai"].nunique()
                        
                        st.info(f"""
                        **Statistiques d√©tect√©es:**
                        - Pi√®ces uniques: {unique_parts}
                        - Op√©rateurs uniques: {unique_ops}
                        - Essais uniques: {unique_trials}
                        - Nombre total de mesures: {len(df_converted)}
                        """)
                        
                        # V√©rifier l'√©quilibre
                        counts = df_converted.groupby(["Pi√®ce", "Op√©rateur"])["Mesure"].count()
                        if counts.nunique() == 1:
                            st.success(f"‚úÖ Plan √©quilibr√©: {counts.iloc[0]} mesures par couple Pi√®ce√óOp√©rateur")
                        else:
                            st.warning(f"‚ö†Ô∏è Plan non √©quilibr√©: nombre de mesures varie entre {counts.min()} et {counts.max()}")
                        
                        # Mettre √† jour les donn√©es de session
                        st.session_state["df_data"] = df_converted
                        
                except Exception as conv_e:
                    st.error(f"‚ùå Erreur lors de la conversion: {str(conv_e)}")
                    st.info("Essayez de r√©organiser vos donn√©es dans le format long standard (Pi√®ce, Op√©rateur, Essai, Mesure) et r√©importez-les.")
                    
            except Exception as e:
                st.error(f"‚ùå Erreur lors de la lecture du fichier: {str(e)}")
                st.info("Veuillez v√©rifier que le fichier est au bon format et non corrompu.")

        # Afficher l'√©diteur de donn√©es
        st.markdown("### Aper√ßu (vous pouvez corriger/compl√©ter si besoin) :")
        
        # V√©rifier si nous avons des donn√©es
        if st.session_state["df_data"].empty or st.session_state["df_data"]["Mesure"].isna().all():
            st.warning("‚ö†Ô∏è Aucune donn√©e de mesure n'est pr√©sente. Veuillez importer un fichier ou saisir manuellement les donn√©es.")
        else:
            # Compter les valeurs non-nulles
            non_null_count = st.session_state["df_data"]["Mesure"].notna().sum()
            st.info(f"üìä {non_null_count} mesures valides sur {len(st.session_state['df_data'])}")
        
        edited = st.data_editor(
            st.session_state["df_data"],
            use_container_width=True,
            hide_index=True,
            key="editor_import",
            column_config={
                "Pi√®ce": st.column_config.TextColumn("Pi√®ce", width="small"),
                "Op√©rateur": st.column_config.TextColumn("Op√©rateur", width="small"),
                "Essai": st.column_config.NumberColumn("Essai", width="small", min_value=1, max_value=10),
                "Mesure": st.column_config.NumberColumn("Mesure", width="medium", format="%.3f")
            }
        )
        st.session_state["df_data"] = edited

        template_csv = build_template(*shape).to_csv(index=False).encode("utf-8")
        st.download_button("‚¨áÔ∏è T√©l√©charger un template CSV", data=template_csv, file_name="template_gage_rr.csv", mime="text/csv")

    st.divider()
    df_current = st.session_state["df_data"].copy()
    ok, errs = validate_dataset(df_current, int(n_parts), int(n_ops), int(n_trials))
    if ok:
        st.success("‚úÖ Donn√©es valides (plan √©quilibr√©). Vous pouvez consulter les r√©sultats.")
    else:
        st.error("‚ùå Donn√©es invalides :")
        for e in errs:
            st.write(f"- {e}")
        
        # Afficher des diagnostics suppl√©mentaires
        if not df_current.empty and "Mesure" in df_current.columns:
            st.markdown("**Diagnostic d√©taill√©:**")
            
            # V√©rifier les comptes par groupe
            if {"Pi√®ce", "Op√©rateur"}.issubset(set(df_current.columns)):
                counts = df_current.groupby(["Pi√®ce", "Op√©rateur"])["Mesure"].count()
                unique_counts = counts.unique()
                
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("Pi√®ces uniques", df_current["Pi√®ce"].nunique())
                with col2:
                    st.metric("Op√©rateurs uniques", df_current["Op√©rateur"].nunique())
                
                st.write("**Distribution des mesures par couple:**")
                st.dataframe(counts.reset_index().rename(columns={"Mesure": "Nombre de mesures"}), use_container_width=True)

# --- Calcul (si donn√©es valides)
res: Optional[AnovaResult] = None
df_for_calc: Optional[pd.DataFrame] = None
ok, errs = validate_dataset(st.session_state["df_data"], int(n_parts), int(n_ops), int(n_trials))
if ok:
    df_for_calc = st.session_state["df_data"].copy()
    try:
        res = gage_rr_anova(df_for_calc, confidence_level)
    except Exception as e:
        res = None
        with tabs[1]:
            st.error(f"Erreur de calcul: {e}")

# --- Tab 2 : R√©sultats
with tabs[1]:
    st.subheader("R√©sultats (EV, AV, Vp, Vt, %R&R)")
    if res is None:
        st.info("Renseignez/importe des donn√©es valides pour voir les r√©sultats.")
    else:
        # Affichage du niveau de confiance
        st.markdown(f"**Niveau de confiance utilis√© : {res.conclusion['confidence_level']}**")
        st.markdown(f"**{res.conclusion['interaction_decision']}**")
        
        c1, c2, c3, c4, c5 = st.columns(5)
        c1.metric("EV", _fmt(res.metrics["EV"]), help="Repeatability (6œÉ)")
        c2.metric("AV", _fmt(res.metrics["AV"]), help="Reproducibility (6œÉ) incluant interaction")
        c3.metric("Vp (PV)", _fmt(res.metrics["PV"]), help="Part-to-Part (6œÉ)")
        c4.metric("Vt (TV)", _fmt(res.metrics["TV"]), help="Total Variation (6œÉ)")
        c5.metric("%R&R", f"{res.metrics['%GRR']:.2f}%", help="%Study Variation = GRR / TV")

        st.markdown(
            f"""
<div class="rr-card {res.conclusion['css']}">
  <h3>Interpr√©tation (comme votre r√®gle)</h3>
  <p><b>%R&amp;R = {res.metrics['%GRR']:.2f}%</b></p>
  <p>{res.conclusion['regle']}</p>
  <p><b>Niveau :</b> {res.conclusion['niveau']} &nbsp;&nbsp;|&nbsp;&nbsp; <b>ndc :</b> {res.metrics['ndc']}</p>
</div>
""",
            unsafe_allow_html=True,
        )

        if tol and tol > 0:
            pct_tol = (res.metrics["GRR"] / tol) * 100
            st.write(f"**%Tol√©rance (GRR/Tol)** ‚âà **{pct_tol:.2f}%**")

        st.divider()

        colA, colB = st.columns([1, 1])
        with colA:
            st.markdown("### %Study Variation (6œÉ)")
            st.dataframe(res.study_var, use_container_width=True, hide_index=True)

        with colB:
            st.markdown("### Table ANOVA (avec tests F)")
            st.dataframe(res.anova_table, use_container_width=True, hide_index=True)

        st.markdown("### Composantes de variance")
        st.dataframe(res.var_components, use_container_width=True, hide_index=True)

# --- Tab 3 : Graphes
with tabs[2]:
    st.subheader("Graphes")
    if res is None or df_for_calc is None:
        st.info("Renseignez/importe des donn√©es valides pour voir les graphes.")
    else:
        g1, g2 = st.columns(2)
        with g1:
            st.pyplot(fig_boxplot(df_for_calc, by="Op√©rateur", title="Boxplot par op√©rateur"), clear_figure=True)
        with g2:
            st.pyplot(fig_boxplot(df_for_calc, by="Pi√®ce", title="Boxplot par pi√®ce"), clear_figure=True)

        g3, g4 = st.columns(2)
        with g3:
            st.pyplot(fig_interaction(df_for_calc), clear_figure=True)
        with g4:
            st.pyplot(fig_studyvar(res.study_var), clear_figure=True)

        st.pyplot(fig_contribution(res.var_components), clear_figure=True)
        st.pyplot(fig_pvalues(res.anova_table), clear_figure=True)

# --- Tab 4 : Rapport / Export
with tabs[3]:
    st.subheader("Rapport & exports")
    if res is None or df_for_calc is None:
        st.info("Renseignez/importe des donn√©es valides pour g√©n√©rer le rapport.")
    else:
        col1, col2 = st.columns([1, 1])

        with col1:
            st.markdown("#### üìÑ Rapport PDF (avec graphes)")
            pdf_bytes = generate_pdf_report(df_for_calc, res, tol=(tol if tol > 0 else None))
            st.download_button(
                "‚¨áÔ∏è T√©l√©charger le rapport PDF",
                data=pdf_bytes,
                file_name="rapport_gage_rr.pdf",
                mime="application/pdf",
                use_container_width=True,
            )
            st.markdown("<div class='small-note'>Le PDF inclut: param√®tres, conclusion, tableaux ANOVA/variance, tests F, et graphes.</div>", unsafe_allow_html=True)

        with col2:
            st.markdown("#### üìä Export Excel (donn√©es + r√©sultats)")
            xlsx_bytes = export_excel(df_for_calc, res)
            st.download_button(
                "‚¨áÔ∏è T√©l√©charger Excel",
                data=xlsx_bytes,
                file_name="resultats_gage_rr.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True,
            )

        st.divider()
        st.markdown("#### Export des donn√©es")
        st.download_button(
            "‚¨áÔ∏è T√©l√©charger les donn√©es (CSV)",
            data=df_for_calc.to_csv(index=False).encode("utf-8"),
            file_name="donnees_gage_rr.csv",
            mime="text/csv",
            use_container_width=True,
        )

st.markdown("<hr/>", unsafe_allow_html=True)
st.markdown(
    "<div class='small-note'>‚ö†Ô∏è Cette app suppose un plan <b>√©quilibr√©</b> (m√™me nombre d'essais pour chaque couple Pi√®ce√óOp√©rateur). "
    f"Calculs effectu√©s avec un niveau de confiance de {confidence_level*100:.0f}% pour les tests d'hypoth√®se.</div>",
    unsafe_allow_html=True,
)
